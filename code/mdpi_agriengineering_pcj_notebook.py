# -*- coding: utf-8 -*-
"""mdpi_agriengineering_pcj_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/rfsilva1/data-driven-water-quality/blob/main/code/mdpi_agriengineering_pcj_notebook.ipynb

<b>A Data-Driven Method for Water Quality Analysis and Prediction for Localized Irrigation</b>

<b>Authors</b>: Silva, R.F.; Benso, M.R.; Corrêa, F.E.; Messias, T.G.; Mendonça, F.C.; Marques, P.A.A.; Duarte, S.N.; Mendiondo, E.M.; Delbem A.C.B.; Saraiva, A.M.

<b>Link github repository</b>: https://github.com/rfsilva1/data-driven-water-quality

<b>Link paper</b>: https://www.mdpi.com/2624-7402/6/2/103

<b>Code implementation</b>: Silva, R.F.

<b>Code review</b>: Benso, M.R.

<b>Description code</b>: implement the proposed method for the PCJ basin, Brazil. It can be adapted to use different functions, models, and evaluate water quality in different areas

<b>Observations</b>:
- A high-RAM machine is recommended. With Colab Pro high-RAM, the code runs in around 5min (the original dataset has 741 data points (rows) and 13 columns)
- Using Google Drive to store files is recommended. If so, it must be mounted
"""

import argparse
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.preprocessing import StandardScaler
import geopandas as gpd
from shapely.geometry import Point
from sklearn.preprocessing import MinMaxScaler
from matplotlib.patches import Patch

import warnings
warnings.filterwarnings("ignore")

def mount_google_drive():
    """Mounts Google Drive to the Colab environment."""
    from google.colab import drive
    drive.mount('/content/drive')

CONFIG = {
    "ids_to_drop": [
        '2011_1', '2011_2', '2011_5', '2011_6', '2011_7', '2011_8',
        '2015_10', '2015_11', '2015_12', '2016_1', '2016_2', '2016_3',
        '2016_5', '2016_6', '2016_7', '2016_8', '2016_11', '2016_12',
        '2017_1', '2017_2', '2017_5', '2017_6', '2017_7', '2017_8',
        '2017_11', '2017_12'
    ],
    "flood_year": ['2011_11', '2011_12', '2012_1', '2012_2', '2012_5', '2012_6', '2012_7', '2012_8'],
    "normal_year": ['2012_11', '2012_12', '2013_1', '2013_2', '2013_5', '2013_6', '2013_7', '2013_8'],
    "drought_year": ['2014_10', '2014_11', '2014_12', '2015_1', '2015_2', '2015_3', '2015_4', '2015_5', '2015_6', '2015_7', '2015_8'],
    "variables_of_interest": ['conductivity', 'iron_total', 'ph', 'hardness', 'dbo', 'ctt', 'ctt_original'],
    "cluster_value_colors": {0: 'fuchsia', 1: 'orange', 2: 'black', 3: 'purple', 4: 'orange'},
    "variable_colors": {
        'conductivity': 'green',
        'ctt_original': 'blue',
        'dbo': 'purple',
        'hardness': 'orange',
        'ph': 'black',
        'iron_total': 'red',
    }
}

def load_and_preprocess_data(data_path):
    """Loads and preprocesses the raw data."""
    raw_df = pd.read_csv(data_path)

    column_dictionary = {
        'Alcalinidade Total': 'alcalinity',
        'Alumínio Dissolvido': 'aluminum',
        'Condutividade': 'conductivity',
        'Ferro Total': 'iron_total',
        'pH': 'ph',
        'Dureza': 'hardness',
        'DBO (5, 20)': 'dbo',
        'CTt': 'ctt'
    }
    raw_df.rename(columns=column_dictionary, inplace=True)
    raw_df.info()
    raw_df.describe()
    return raw_df

def classify_ctt(value):
    if value < 10000:
        return 0
    elif value >= 10000 and value < 50000:
        return 1
    else:
        return 2

def clean_data(raw_df, config):
    """Cleans and prepares the data for analysis."""
    df = raw_df[['year','month','conductivity','iron_total','ph','hardness','dbo','ctt']].copy()
    df['ctt_original'] = df['ctt']
    df['ctt'] = df['ctt'].apply(classify_ctt)
    df['season'] = df['year'].astype(str) + '_' + df['month'].astype(str)
    df.drop(['year', 'month'], axis=1, inplace=True)
    ids_to_drop = config['ids_to_drop']
    mask = ~df['season'].isin(ids_to_drop)
    filtered_df = df[mask]
    return filtered_df, df

def create_hydrological_year_datasets(filtered_df, config):
    """Creates datasets for flood, normal, and drought years."""
    flood = filtered_df[filtered_df['season'].isin(config['flood_year'])].copy()
    normal = filtered_df[filtered_df['season'].isin(config['normal_year'])].copy()
    drought = filtered_df[filtered_df['season'].isin(config['drought_year'])].copy()

    flood['original_index'] = flood.index
    normal['original_index'] = normal.index
    drought['original_index'] = drought.index

    flood = flood.reset_index(drop=True)
    normal = normal.reset_index(drop=True)
    drought = drought.reset_index(drop=True)

    return flood, normal, drought

def create_feature_subsets(flood, normal, drought):
    """Creates feature subsets for clustering."""
    flood_no_index = flood.drop(columns=['original_index'])
    normal_no_index = normal.drop(columns=['original_index'])
    drought_no_index = drought.drop(columns=['original_index'])

    flood_soil_indices = flood[['ph', 'conductivity']]
    flood_irrigation_indices = flood[['iron_total', 'hardness','dbo','ctt']]
    flood_all_indices = flood_no_index.copy()
    flood_all_indices.drop(columns=['ctt_original','season'], inplace=True)

    normal_soil_indices = normal[['ph', 'conductivity']]
    normal_irrigation_indices = normal[['iron_total', 'hardness','dbo','ctt']]
    normal_all_indices = normal_no_index.copy()
    normal_all_indices.drop(columns=['ctt_original','season'], inplace=True)

    drought_soil_indices = drought[['ph', 'conductivity']]
    drought_irrigation_indices = drought[['iron_total', 'hardness','dbo','ctt']]
    drought_all_indices = drought_no_index.copy()
    drought_all_indices.drop(columns=['ctt_original','season'], inplace=True)

    datasets = {
        'flood_soil_indices': flood_soil_indices,
        'flood_irrigation_indices': flood_irrigation_indices,
        'flood_all_indices': flood_all_indices,
        'normal_soil_indices': normal_soil_indices,
        'normal_irrigation_indices': normal_irrigation_indices,
        'normal_all_indices': normal_all_indices,
        'drought_soil_indices': drought_soil_indices,
        'drought_irrigation_indices': drought_irrigation_indices,
        'drought_all_indices': drought_all_indices
    }
    return datasets


def find_optimal_k_silhouette(data, dataset_name):
    """Finds the optimal k for a dataset using the silhouette score."""
    max_silhouette_score = -1
    optimal_k = 2

    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data)

    for k in range(2, 11):
        kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
        labels = kmeans.fit_predict(scaled_data)
        silhouette_avg = silhouette_score(scaled_data, labels)
        if silhouette_avg > max_silhouette_score:
            max_silhouette_score = silhouette_avg
            optimal_k = k
    print(f"Optimal k for {dataset_name} is {optimal_k}")
    return optimal_k


def rename_cluster_labels(data, labels):
    """Renames cluster labels based on the mean of the first feature."""
    data['label'] = labels
    feature_column = data.columns[0]
    cluster_means = data.groupby('label')[feature_column].mean()
    cluster_means_sorted = cluster_means.sort_values().index
    label_map = {cluster_means_sorted[i]: i for i in range(len(cluster_means_sorted))}
    data['label'] = data['label'].map(label_map)
    return data['label']

def run_clustering(datasets):
    """Runs k-means clustering on the datasets, finding the optimal k for each."""
    for dataset_name, dataset in datasets.items():
        optimal_k = find_optimal_k_silhouette(dataset, dataset_name)
        scaler = StandardScaler()
        scaled_features = scaler.fit_transform(dataset)
        kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=42)
        labels = kmeans.fit_predict(scaled_features)
        dataset['label'] = rename_cluster_labels(dataset.copy(), labels)
        dataset.rename(columns={'label': dataset_name + '_label'}, inplace=True)

    return datasets


def add_cluster_labels_to_main_dataframes(clustered_datasets, flood, normal, drought):
    """Adds the cluster labels to the main dataframes."""
    flood_datasets = ['flood_soil_indices', 'flood_irrigation_indices', 'flood_all_indices']
    normal_datasets = ['normal_soil_indices', 'normal_irrigation_indices', 'normal_all_indices']
    drought_datasets = ['drought_soil_indices', 'drought_irrigation_indices', 'drought_all_indices']

    for dataset_name in flood_datasets:
        label_column = dataset_name + '_label'
        flood[label_column] = clustered_datasets[dataset_name][label_column]

    for dataset_name in normal_datasets:
        label_column = dataset_name + '_label'
        normal[label_column] = clustered_datasets[dataset_name][label_column]

    for dataset_name in drought_datasets:
        label_column = dataset_name + '_label'
        drought[label_column] = clustered_datasets[dataset_name][label_column]

    return flood, normal, drought

def add_location_data(flood, normal, drought, raw_df, df):
    """Adds location data back to the dataframes."""
    columns_to_add = ['latitude_decimal', 'longitude_decimal', 'cod_interaguas']
    season_column = 'season'

    for dataset in [flood, normal, drought]:
        original_indices = dataset['original_index']
        selected_raw_df = raw_df.loc[original_indices, columns_to_add]
        selected_df = df.loc[original_indices, [season_column]]
        dataset[columns_to_add] = selected_raw_df.values
        dataset[season_column] = selected_df[season_column].values

    return flood, normal, drought

def create_geo_dataframes(flood, normal, drought, shapefile_path):
    """Converts dataframes to GeoDataFrames."""
    shapefile = gpd.read_file(shapefile_path)

    geometry = [Point(xy) for xy in zip(flood['longitude_decimal'], flood['latitude_decimal'])]
    flood_gdf = gpd.GeoDataFrame(flood, geometry=geometry, crs=shapefile.crs)

    geometry = [Point(xy) for xy in zip(normal['longitude_decimal'], normal['latitude_decimal'])]
    normal_gdf = gpd.GeoDataFrame(normal, geometry=geometry, crs=shapefile.crs)

    geometry = [Point(xy) for xy in zip(drought['longitude_decimal'], drought['latitude_decimal'])]
    drought_gdf = gpd.GeoDataFrame(drought, geometry=geometry, crs=shapefile.crs)

    return flood_gdf, normal_gdf, drought_gdf, shapefile


def majority_value(series):
    return series.mode().iloc[0]

def list_of_values(series):
    return series.tolist()

def create_seasonal_datasets(flood_gdf, normal_gdf, drought_gdf):
    """Creates seasonal datasets."""
    flood_gdf_s1 = flood_gdf[flood_gdf['season'].isin(['2011_11', '2011_12', '2012_1', '2012_2'])]
    flood_gdf_s2 = flood_gdf[flood_gdf['season'].isin(['2012_5', '2012_6', '2012_7', '2012_8'])]
    normal_gdf_s1 = normal_gdf[normal_gdf['season'].isin(['2012_11', '2012_12', '2013_1', '2013_2'])]
    normal_gdf_s2 = normal_gdf[normal_gdf['season'].isin(['2013_5', '2013_6', '2013_7', '2013_8'])]
    drought_gdf_s1 = drought_gdf[drought_gdf['season'].isin(['2014_10', '2014_11', '2014_12', '2015_1', '2015_2', '2015_3'])]
    drought_gdf_s2 = drought_gdf[drought_gdf['season'].isin(['2015_4', '2015_5', '2015_6', '2015_7', '2015_8'])]
    return flood_gdf_s1, flood_gdf_s2, normal_gdf_s1, normal_gdf_s2, drought_gdf_s1, drought_gdf_s2

def aggregate_data_by_station(flood_gdf, normal_gdf, drought_gdf):
    """Aggregates data by station."""
    def get_agg_dict(prefix):
        return {
            'latitude_decimal': 'mean',
            'longitude_decimal': 'mean',
            'geometry': majority_value,
            'conductivity': 'mean',
            'iron_total': 'mean',
            'ph': 'mean',
            'hardness': 'mean',
            'ctt': majority_value,
            'ctt_original': 'mean',
            'dbo': 'mean',
            f'{prefix}_soil_indices_label': majority_value,
            f'{prefix}_irrigation_indices_label': majority_value,
            f'{prefix}_all_indices_label': majority_value,
            'original_index': list_of_values
        }

    flood_gdf = flood_gdf.groupby('cod_interaguas').agg(get_agg_dict('flood')).reset_index()
    normal_gdf = normal_gdf.groupby('cod_interaguas').agg(get_agg_dict('normal')).reset_index()
    drought_gdf = drought_gdf.groupby('cod_interaguas').agg(get_agg_dict('drought')).reset_index()

    return flood_gdf, normal_gdf, drought_gdf

def plot_cluster_maps(datasets, shapefile):
    """Plots maps of the clustering results."""
    value_colors = {0: 'fuchsia', 1: 'orange', 2: 'black', 3: 'purple', 4: 'orange'}
    fig, axs = plt.subplots(3, 3, figsize=(15, 15))
    dataset_names = list(datasets.keys())
    for i, (dataset_name, gdf) in enumerate(datasets.items()):
        for j, variable in enumerate(datasets[dataset_name]):
            ax = axs[i, j]
            shapefile.plot(ax=ax, color='lightgrey', alpha=0.5, zorder=1)
            circle_size = 50
            existing_values = gdf[variable].unique()
            for value in existing_values:
                color = value_colors.get(value)
                if color:
                    filter_values = gdf[gdf[variable] == value]
                    ax.scatter(filter_values.geometry.x, filter_values.geometry.y, color=color, label=f'Cluster {value}', s=circle_size, zorder=2)
            ax.set_title(f'Clustering: {variable} - {dataset_name}')
            handles, labels = ax.get_legend_handles_labels()
            labels, handles = zip(*sorted(zip(labels, handles), key=lambda x: int(x[0].split()[1])))
            ax.legend(handles, labels, title='Cluster group', loc='upper right')
    plt.tight_layout()
    plt.show()

def plot_variable_maps(gdfs, shapefile, config):
    """Plots maps of the variables."""
    variables = config['variables_of_interest']
    colors = config['variable_colors']
    quantiles = [0, 25, 50, 75, 100]

    for var in variables:
        if var not in colors:
            continue
        fig, axs = plt.subplots(3, 3, figsize=(15, 15))
        for i, (gdf_name, gdf) in enumerate(gdfs.items()):
            ax = axs[i // 3, i % 3]
            shapefile.plot(ax=ax, color='lightblue', alpha=0.3, zorder=1)
            circle_sizes = np.linspace(10, 50, len(quantiles) - 1)
            if len(quantiles) >= 2:
                for k in range(len(quantiles) - 1):
                    lower_bound = np.percentile(gdf[var], quantiles[k])
                    upper_bound = np.percentile(gdf[var], quantiles[k + 1])
                    subset = gdf[(gdf[var] >= lower_bound) & (gdf[var] <= upper_bound)]
                    circle_size = circle_sizes[k]
                    ax.scatter(subset.geometry.x, subset.geometry.y, color=colors[var],
                               label=f'{quantiles[k]}-{quantiles[k + 1]}%', s=circle_size, zorder=2)
            ax.set_title(f'Quantiles of {var} - {gdf_name}')
        plt.subplots_adjust(bottom=0.05)
        plt.show()

def generate_summary_statistics(gdfs, config):
    """Generates a dataframe with summary statistics."""
    variables = config['variables_of_interest']
    results = []
    for dataset_name, dataset in gdfs.items():
        for variable in variables:
            mean_val = dataset[variable].mean()
            std_val = dataset[variable].std()
            coef_var = (std_val / mean_val) * 100 if mean_val != 0 else 0
            min_val = dataset[variable].min()
            max_val = dataset[variable].max()
            results.append({'Dataset': dataset_name, 'Variable': variable, 'Statistic': 'mean', 'Value': mean_val})
            results.append({'Dataset': dataset_name, 'Variable': variable, 'Statistic': 'std', 'Value': std_val})
            results.append({'Dataset': dataset_name, 'Variable': variable, 'Statistic': 'coef_var', 'Value': coef_var})
            results.append({'Dataset': dataset_name, 'Variable': variable, 'Statistic': 'min', 'Value': min_val})
            results.append({'Dataset': dataset_name, 'Variable': variable, 'Statistic': 'max', 'Value': max_val})
    stats_df = pd.DataFrame(results)
    return stats_df

def main(data_path, shapefile_path):
    """
    Main function to run the water quality analysis.
    """
    raw_df = load_and_preprocess_data(data_path)
    filtered_df, df = clean_data(raw_df, CONFIG)
    flood, normal, drought = create_hydrological_year_datasets(filtered_df, CONFIG)
    datasets = create_feature_subsets(flood, normal, drought)
    clustered_datasets = run_clustering(datasets)
    flood, normal, drought = add_cluster_labels_to_main_dataframes(clustered_datasets, flood, normal, drought)
    flood, normal, drought = add_location_data(flood, normal, drought, raw_df, df)

    # The following lines are commented out because they require the shapefile, which is too large to be unzipped in this environment.
    # flood_gdf, normal_gdf, drought_gdf, shapefile = create_geo_dataframes(flood, normal, drought, shapefile_path)
    # flood_gdf_s1, flood_gdf_s2, normal_gdf_s1, normal_gdf_s2, drought_gdf_s1, drought_gdf_s2 = create_seasonal_datasets(flood_gdf, normal_gdf, drought_gdf)
    # flood_gdf, normal_gdf, drought_gdf = aggregate_data_by_station(flood_gdf, normal_gdf, drought_gdf)

    # cluster_plot_datasets = {
    #     'flood_gdf': flood_gdf,
    #     'normal_gdf': normal_gdf,
    #     'drought_gdf': drought_gdf
    # }
    # plot_cluster_maps(cluster_plot_datasets, shapefile, CONFIG)

    # gdfs_for_variable_maps = {
    #     'flood': flood_gdf, 'flood_s1': flood_gdf_s1, 'flood_s2': flood_gdf_s2,
    #     'normal': normal_gdf, 'normal_s1': normal_gdf_s1, 'normal_s2': normal_gdf_s2,
    #     'drought': drought_gdf, 'drought_s1': drought_gdf_s1, 'drought_s2': drought_gdf_s2
    # }
    # plot_variable_maps(gdfs_for_variable_maps, shapefile, CONFIG)

    # stats_df = generate_summary_statistics(gdfs_for_variable_maps, CONFIG)
    # print(stats_df)


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='A Data-Driven Method for Water Quality Analysis and Prediction for Localized Irrigation.')
    parser.add_argument('--data_path', type=str, default='../dataset/data_final_qualidade_pcj_v3.csv', help='Path to the dataset CSV file.')
    parser.add_argument('--shapefile_path', type=str, default='../dataset/GEOFT_BHO_PCJ_TDR-20240613T175444Z-001/GEOFT_BHO_PCJ_TDR.shp', help='Path to the shapefile.')
    args = parser.parse_args()
    main(args.data_path, args.shapefile_path)